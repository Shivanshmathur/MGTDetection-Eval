{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shivansh Mathur\\.conda\\envs\\dlproj\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the dataset from a JSONL file\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Preprocess and tokenize the data\n",
    "def preprocess_data(data, tokenizer, max_length=512):\n",
    "    tokenized = tokenizer(data['text'].tolist(), padding='max_length', max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preprocessed_data(tokenized_inputs, labels, file_path='preprocessed_data.pt'):\n",
    "    torch.save({\n",
    "        'input_ids': tokenized_inputs['input_ids'],\n",
    "        'attention_mask': tokenized_inputs['attention_mask'],\n",
    "        'labels': labels\n",
    "    }, file_path)\n",
    "\n",
    "def load_preprocessed_data(file_path='preprocessed_data.pt'):\n",
    "    data = torch.load(file_path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and prepare data\n",
    "train_data = load_data('data/subtaskA/subtaskA_train_monolingual.jsonl')\n",
    "tokenized_inputs = preprocess_data(train_data, tokenizer)\n",
    "labels = torch.tensor(train_data['label'].tolist()).unsqueeze(1)\n",
    "\n",
    "# Save the preprocessed data\n",
    "save_preprocessed_data(tokenized_inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the preprocessed data\n",
    "preprocessed_data = load_preprocessed_data()\n",
    "# Create TensorDataset and DataLoader\n",
    "#dataset = TensorDataset(tokenized_inputs['input_ids'], tokenized_inputs['attention_mask'], labels)\n",
    "dataset = TensorDataset(preprocessed_data['input_ids'], preprocessed_data['attention_mask'], preprocessed_data['labels'])\n",
    "data_loader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "# Define optimizer and loss\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, data_loader, optimizer, device, epochs=1):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 20 == 0:  # Log every 100 batches\n",
    "                print(f'Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {loss.item()}')\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f'End of Epoch {epoch+1}, Average Loss: {avg_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 0, Loss: 0.7237396240234375\n",
      "Epoch: 1, Batch: 20, Loss: 0.593963623046875\n",
      "Epoch: 1, Batch: 40, Loss: 0.5186614990234375\n",
      "Epoch: 1, Batch: 60, Loss: 0.34515380859375\n",
      "Epoch: 1, Batch: 80, Loss: 0.09571170806884766\n",
      "Epoch: 1, Batch: 100, Loss: 0.39281463623046875\n",
      "Epoch: 1, Batch: 120, Loss: 0.42992591857910156\n",
      "Epoch: 1, Batch: 140, Loss: 0.2562241554260254\n",
      "Epoch: 1, Batch: 160, Loss: 0.0980074405670166\n",
      "Epoch: 1, Batch: 180, Loss: 0.3342905044555664\n",
      "Epoch: 1, Batch: 200, Loss: 0.6742444038391113\n",
      "Epoch: 1, Batch: 220, Loss: 0.37153661251068115\n",
      "Epoch: 1, Batch: 240, Loss: 0.19369721412658691\n",
      "Epoch: 1, Batch: 260, Loss: 0.5417435169219971\n",
      "Epoch: 1, Batch: 280, Loss: 0.2115495204925537\n",
      "Epoch: 1, Batch: 300, Loss: 0.11365318298339844\n",
      "Epoch: 1, Batch: 320, Loss: 0.392169713973999\n",
      "Epoch: 1, Batch: 340, Loss: 0.17237114906311035\n",
      "Epoch: 1, Batch: 360, Loss: 0.4928017854690552\n",
      "Epoch: 1, Batch: 380, Loss: 0.09472620487213135\n",
      "Epoch: 1, Batch: 400, Loss: 0.49457454681396484\n",
      "Epoch: 1, Batch: 420, Loss: 0.3138720989227295\n",
      "Epoch: 1, Batch: 440, Loss: 0.24736690521240234\n",
      "Epoch: 1, Batch: 460, Loss: 0.4378281235694885\n",
      "Epoch: 1, Batch: 480, Loss: 0.004230797290802002\n",
      "Epoch: 1, Batch: 500, Loss: 0.14248454570770264\n",
      "Epoch: 1, Batch: 520, Loss: 0.11560773849487305\n",
      "Epoch: 1, Batch: 540, Loss: 0.019370317459106445\n",
      "Epoch: 1, Batch: 560, Loss: 0.012655138969421387\n",
      "Epoch: 1, Batch: 580, Loss: 0.04330849647521973\n",
      "Epoch: 1, Batch: 600, Loss: 0.05998891592025757\n",
      "Epoch: 1, Batch: 620, Loss: 0.03461161255836487\n",
      "Epoch: 1, Batch: 640, Loss: 0.11355674266815186\n",
      "Epoch: 1, Batch: 660, Loss: 0.06923490762710571\n",
      "Epoch: 1, Batch: 680, Loss: 0.0800439715385437\n",
      "Epoch: 1, Batch: 700, Loss: 0.05812716484069824\n",
      "Epoch: 1, Batch: 720, Loss: 0.01738131046295166\n",
      "Epoch: 1, Batch: 740, Loss: 0.5267062783241272\n",
      "Epoch: 1, Batch: 760, Loss: 0.249350905418396\n",
      "Epoch: 1, Batch: 780, Loss: 0.24051818251609802\n",
      "Epoch: 1, Batch: 800, Loss: 0.230349600315094\n",
      "Epoch: 1, Batch: 820, Loss: 0.2955583333969116\n",
      "Epoch: 1, Batch: 840, Loss: 0.21145468950271606\n",
      "Epoch: 1, Batch: 860, Loss: 0.19248509407043457\n",
      "Epoch: 1, Batch: 880, Loss: 0.12275227904319763\n",
      "Epoch: 1, Batch: 900, Loss: 0.02531123161315918\n",
      "Epoch: 1, Batch: 920, Loss: 0.12017643451690674\n",
      "Epoch: 1, Batch: 940, Loss: 0.286579966545105\n",
      "Epoch: 1, Batch: 960, Loss: 0.3518572151660919\n",
      "Epoch: 1, Batch: 980, Loss: 0.02345806360244751\n",
      "Epoch: 1, Batch: 1000, Loss: 0.1512969732284546\n",
      "Epoch: 1, Batch: 1020, Loss: 0.09763568639755249\n",
      "Epoch: 1, Batch: 1040, Loss: 0.11078393459320068\n",
      "Epoch: 1, Batch: 1060, Loss: 0.400930792093277\n",
      "Epoch: 1, Batch: 1080, Loss: 0.16991513967514038\n",
      "Epoch: 1, Batch: 1100, Loss: 0.030622273683547974\n",
      "Epoch: 1, Batch: 1120, Loss: 0.04772338271141052\n",
      "Epoch: 1, Batch: 1140, Loss: 0.12525933980941772\n",
      "Epoch: 1, Batch: 1160, Loss: 0.06037144362926483\n",
      "Epoch: 1, Batch: 1180, Loss: 0.07267609238624573\n",
      "Epoch: 1, Batch: 1200, Loss: 0.26674002408981323\n",
      "Epoch: 1, Batch: 1220, Loss: 0.030208498239517212\n",
      "Epoch: 1, Batch: 1240, Loss: 0.1082586795091629\n",
      "Epoch: 1, Batch: 1260, Loss: 0.07675500214099884\n",
      "Epoch: 1, Batch: 1280, Loss: 0.07662594318389893\n",
      "Epoch: 1, Batch: 1300, Loss: 0.11682519316673279\n",
      "Epoch: 1, Batch: 1320, Loss: 0.12106665968894958\n",
      "Epoch: 1, Batch: 1340, Loss: 0.05178350210189819\n",
      "Epoch: 1, Batch: 1360, Loss: 0.29767856001853943\n",
      "Epoch: 1, Batch: 1380, Loss: 0.08499971032142639\n",
      "Epoch: 1, Batch: 1400, Loss: 0.03962017595767975\n",
      "Epoch: 1, Batch: 1420, Loss: 0.005407243967056274\n",
      "Epoch: 1, Batch: 1440, Loss: 0.3515738248825073\n",
      "Epoch: 1, Batch: 1460, Loss: 0.20757120847702026\n",
      "Epoch: 1, Batch: 1480, Loss: 0.05366729199886322\n",
      "Epoch: 1, Batch: 1500, Loss: 0.08101683855056763\n",
      "Epoch: 1, Batch: 1520, Loss: 0.21789231896400452\n",
      "Epoch: 1, Batch: 1540, Loss: 0.02018316090106964\n",
      "Epoch: 1, Batch: 1560, Loss: 0.04656118154525757\n",
      "Epoch: 1, Batch: 1580, Loss: 0.015264511108398438\n",
      "Epoch: 1, Batch: 1600, Loss: 0.07808583974838257\n",
      "Epoch: 1, Batch: 1620, Loss: 0.019313648343086243\n",
      "Epoch: 1, Batch: 1640, Loss: 0.11529801040887833\n",
      "Epoch: 1, Batch: 1660, Loss: 0.03924226760864258\n",
      "Epoch: 1, Batch: 1680, Loss: 0.028771229088306427\n",
      "Epoch: 1, Batch: 1700, Loss: 0.04777961224317551\n",
      "Epoch: 1, Batch: 1720, Loss: 0.02303329110145569\n",
      "Epoch: 1, Batch: 1740, Loss: 0.1723971664905548\n",
      "Epoch: 1, Batch: 1760, Loss: 0.010760575532913208\n",
      "Epoch: 1, Batch: 1780, Loss: 0.1634952425956726\n",
      "Epoch: 1, Batch: 1800, Loss: 0.3202595114707947\n",
      "Epoch: 1, Batch: 1820, Loss: 0.10293892025947571\n",
      "Epoch: 1, Batch: 1840, Loss: 0.11324505507946014\n",
      "Epoch: 1, Batch: 1860, Loss: 0.20575642585754395\n",
      "Epoch: 1, Batch: 1880, Loss: 0.09957098960876465\n",
      "Epoch: 1, Batch: 1900, Loss: 0.021146297454833984\n",
      "Epoch: 1, Batch: 1920, Loss: 0.09443359076976776\n",
      "Epoch: 1, Batch: 1940, Loss: 0.013102009892463684\n",
      "Epoch: 1, Batch: 1960, Loss: 0.24304570257663727\n",
      "Epoch: 1, Batch: 1980, Loss: 0.008983761072158813\n",
      "Epoch: 1, Batch: 2000, Loss: 0.2388153374195099\n",
      "Epoch: 1, Batch: 2020, Loss: 0.12606684863567352\n",
      "Epoch: 1, Batch: 2040, Loss: 0.026478111743927002\n",
      "Epoch: 1, Batch: 2060, Loss: 0.024772748351097107\n",
      "Epoch: 1, Batch: 2080, Loss: 0.0027357935905456543\n",
      "Epoch: 1, Batch: 2100, Loss: 0.13959147036075592\n",
      "Epoch: 1, Batch: 2120, Loss: 0.020484119653701782\n",
      "Epoch: 1, Batch: 2140, Loss: 0.030804336071014404\n",
      "Epoch: 1, Batch: 2160, Loss: 0.018537580966949463\n",
      "Epoch: 1, Batch: 2180, Loss: 0.04675284028053284\n",
      "Epoch: 1, Batch: 2200, Loss: 0.0051638782024383545\n",
      "Epoch: 1, Batch: 2220, Loss: 0.0317247211933136\n",
      "Epoch: 1, Batch: 2240, Loss: 0.17400206625461578\n",
      "Epoch: 1, Batch: 2260, Loss: 0.04097810387611389\n",
      "Epoch: 1, Batch: 2280, Loss: 0.031125172972679138\n",
      "Epoch: 1, Batch: 2300, Loss: 0.01178056001663208\n",
      "Epoch: 1, Batch: 2320, Loss: 0.06287840008735657\n",
      "Epoch: 1, Batch: 2340, Loss: 0.031975775957107544\n",
      "Epoch: 1, Batch: 2360, Loss: 0.029179811477661133\n",
      "Epoch: 1, Batch: 2380, Loss: 0.06435750424861908\n",
      "Epoch: 1, Batch: 2400, Loss: 0.007862508296966553\n",
      "Epoch: 1, Batch: 2420, Loss: 0.05739310383796692\n",
      "Epoch: 1, Batch: 2440, Loss: 0.04977503418922424\n",
      "Epoch: 1, Batch: 2460, Loss: 0.3793090879917145\n",
      "Epoch: 1, Batch: 2480, Loss: 0.008010387420654297\n",
      "Epoch: 1, Batch: 2500, Loss: 0.018233396112918854\n",
      "Epoch: 1, Batch: 2520, Loss: 0.23743358254432678\n",
      "Epoch: 1, Batch: 2540, Loss: 0.029917597770690918\n",
      "Epoch: 1, Batch: 2560, Loss: 0.0073610395193099976\n",
      "Epoch: 1, Batch: 2580, Loss: 0.061728090047836304\n",
      "Epoch: 1, Batch: 2600, Loss: 0.030246838927268982\n",
      "Epoch: 1, Batch: 2620, Loss: 0.06291353702545166\n",
      "Epoch: 1, Batch: 2640, Loss: 0.08532679080963135\n",
      "Epoch: 1, Batch: 2660, Loss: 0.00686076283454895\n",
      "Epoch: 1, Batch: 2680, Loss: 0.14534419775009155\n",
      "Epoch: 1, Batch: 2700, Loss: 0.22803038358688354\n",
      "Epoch: 1, Batch: 2720, Loss: 0.26221781969070435\n",
      "Epoch: 1, Batch: 2740, Loss: 0.02680760622024536\n",
      "Epoch: 1, Batch: 2760, Loss: 0.008592844009399414\n",
      "Epoch: 1, Batch: 2780, Loss: 0.015893012285232544\n",
      "Epoch: 1, Batch: 2800, Loss: 0.23320454359054565\n",
      "Epoch: 1, Batch: 2820, Loss: 0.13918450474739075\n",
      "Epoch: 1, Batch: 2840, Loss: 0.02468964457511902\n",
      "Epoch: 1, Batch: 2860, Loss: 0.08622626960277557\n",
      "Epoch: 1, Batch: 2880, Loss: 0.020480066537857056\n",
      "Epoch: 1, Batch: 2900, Loss: 0.04314012825489044\n",
      "Epoch: 1, Batch: 2920, Loss: 0.004488930106163025\n",
      "Epoch: 1, Batch: 2940, Loss: 0.011149987578392029\n",
      "Epoch: 1, Batch: 2960, Loss: 0.07589557766914368\n",
      "Epoch: 1, Batch: 2980, Loss: 0.016022637486457825\n",
      "Epoch: 1, Batch: 3000, Loss: 0.06788623332977295\n",
      "Epoch: 1, Batch: 3020, Loss: 0.23286035656929016\n",
      "Epoch: 1, Batch: 3040, Loss: 0.07949903607368469\n",
      "Epoch: 1, Batch: 3060, Loss: 0.030372068285942078\n",
      "Epoch: 1, Batch: 3080, Loss: 0.019399479031562805\n",
      "Epoch: 1, Batch: 3100, Loss: 0.03193768858909607\n",
      "Epoch: 1, Batch: 3120, Loss: 0.03594830632209778\n",
      "Epoch: 1, Batch: 3140, Loss: 0.009737879037857056\n",
      "Epoch: 1, Batch: 3160, Loss: 0.08390077203512192\n",
      "Epoch: 1, Batch: 3180, Loss: 0.22815600037574768\n",
      "Epoch: 1, Batch: 3200, Loss: 0.2616722881793976\n",
      "Epoch: 1, Batch: 3220, Loss: 0.1105903834104538\n",
      "Epoch: 1, Batch: 3240, Loss: 0.026334598660469055\n",
      "Epoch: 1, Batch: 3260, Loss: 0.094727523624897\n",
      "Epoch: 1, Batch: 3280, Loss: 0.16719037294387817\n",
      "Epoch: 1, Batch: 3300, Loss: 0.10941910743713379\n",
      "Epoch: 1, Batch: 3320, Loss: 0.024747639894485474\n",
      "Epoch: 1, Batch: 3340, Loss: 0.015998050570487976\n",
      "Epoch: 1, Batch: 3360, Loss: 0.05841073393821716\n",
      "Epoch: 1, Batch: 3380, Loss: 0.14796575903892517\n",
      "Epoch: 1, Batch: 3400, Loss: 0.013273432850837708\n",
      "Epoch: 1, Batch: 3420, Loss: 0.08814744651317596\n",
      "Epoch: 1, Batch: 3440, Loss: 0.06918355822563171\n",
      "Epoch: 1, Batch: 3460, Loss: 0.017557621002197266\n",
      "Epoch: 1, Batch: 3480, Loss: 0.011683523654937744\n",
      "Epoch: 1, Batch: 3500, Loss: 0.2449086457490921\n",
      "Epoch: 1, Batch: 3520, Loss: 0.07632184028625488\n",
      "Epoch: 1, Batch: 3540, Loss: 0.11307208240032196\n",
      "Epoch: 1, Batch: 3560, Loss: 0.10780882090330124\n",
      "Epoch: 1, Batch: 3580, Loss: 0.09820234775543213\n",
      "Epoch: 1, Batch: 3600, Loss: 0.02486354112625122\n",
      "Epoch: 1, Batch: 3620, Loss: 0.0018038153648376465\n",
      "Epoch: 1, Batch: 3640, Loss: 0.04937194287776947\n",
      "Epoch: 1, Batch: 3660, Loss: 0.37153130769729614\n",
      "Epoch: 1, Batch: 3680, Loss: 0.05905163288116455\n",
      "Epoch: 1, Batch: 3700, Loss: 0.011545255780220032\n",
      "Epoch: 1, Batch: 3720, Loss: 0.13724622130393982\n",
      "Epoch: 1, Batch: 3740, Loss: 0.11162543296813965\n",
      "Epoch: 1, Batch: 3760, Loss: 0.04687628149986267\n",
      "Epoch: 1, Batch: 3780, Loss: 0.22460931539535522\n",
      "Epoch: 1, Batch: 3800, Loss: 0.5194181203842163\n",
      "Epoch: 1, Batch: 3820, Loss: 0.5269486904144287\n",
      "Epoch: 1, Batch: 3840, Loss: 0.10612055659294128\n",
      "Epoch: 1, Batch: 3860, Loss: 0.04694324731826782\n",
      "Epoch: 1, Batch: 3880, Loss: 0.130812406539917\n",
      "Epoch: 1, Batch: 3900, Loss: 0.025725483894348145\n",
      "Epoch: 1, Batch: 3920, Loss: 0.02580636739730835\n",
      "Epoch: 1, Batch: 3940, Loss: 0.01872764527797699\n",
      "Epoch: 1, Batch: 3960, Loss: 0.11159352213144302\n",
      "Epoch: 1, Batch: 3980, Loss: 0.3413369059562683\n",
      "Epoch: 1, Batch: 4000, Loss: 0.007894173264503479\n",
      "Epoch: 1, Batch: 4020, Loss: 0.33346879482269287\n",
      "Epoch: 1, Batch: 4040, Loss: 0.1632671356201172\n",
      "Epoch: 1, Batch: 4060, Loss: 0.09362807869911194\n",
      "Epoch: 1, Batch: 4080, Loss: 0.20994126796722412\n",
      "Epoch: 1, Batch: 4100, Loss: 0.224664106965065\n",
      "Epoch: 1, Batch: 4120, Loss: 0.119357168674469\n",
      "Epoch: 1, Batch: 4140, Loss: 0.07206885516643524\n",
      "Epoch: 1, Batch: 4160, Loss: 0.1273128092288971\n",
      "Epoch: 1, Batch: 4180, Loss: 0.392148494720459\n",
      "Epoch: 1, Batch: 4200, Loss: 0.11994576454162598\n",
      "Epoch: 1, Batch: 4220, Loss: 0.13096746802330017\n",
      "Epoch: 1, Batch: 4240, Loss: 0.012844949960708618\n",
      "Epoch: 1, Batch: 4260, Loss: 0.005528062582015991\n",
      "Epoch: 1, Batch: 4280, Loss: 0.27967989444732666\n",
      "Epoch: 1, Batch: 4300, Loss: 0.028511345386505127\n",
      "Epoch: 1, Batch: 4320, Loss: 0.018871262669563293\n",
      "Epoch: 1, Batch: 4340, Loss: 0.09584805369377136\n",
      "Epoch: 1, Batch: 4360, Loss: 0.429374635219574\n",
      "Epoch: 1, Batch: 4380, Loss: 0.05779266357421875\n",
      "Epoch: 1, Batch: 4400, Loss: 0.07858327031135559\n",
      "Epoch: 1, Batch: 4420, Loss: 0.08611385524272919\n",
      "Epoch: 1, Batch: 4440, Loss: 0.1202259361743927\n",
      "Epoch: 1, Batch: 4460, Loss: 0.007640808820724487\n",
      "Epoch: 1, Batch: 4480, Loss: 0.004393383860588074\n",
      "Epoch: 1, Batch: 4500, Loss: 0.025089561939239502\n",
      "Epoch: 1, Batch: 4520, Loss: 0.06533642113208771\n",
      "Epoch: 1, Batch: 4540, Loss: 0.050632089376449585\n",
      "Epoch: 1, Batch: 4560, Loss: 0.021549344062805176\n",
      "Epoch: 1, Batch: 4580, Loss: 0.0031694918870925903\n",
      "Epoch: 1, Batch: 4600, Loss: 0.15489506721496582\n",
      "Epoch: 1, Batch: 4620, Loss: 0.2252269983291626\n",
      "Epoch: 1, Batch: 4640, Loss: 0.11329236626625061\n",
      "Epoch: 1, Batch: 4660, Loss: 0.36218541860580444\n",
      "Epoch: 1, Batch: 4680, Loss: 0.06976863741874695\n",
      "Epoch: 1, Batch: 4700, Loss: 0.0320620983839035\n",
      "Epoch: 1, Batch: 4720, Loss: 0.007459118962287903\n",
      "Epoch: 1, Batch: 4740, Loss: 0.010345257818698883\n",
      "Epoch: 1, Batch: 4760, Loss: 0.0505375862121582\n",
      "Epoch: 1, Batch: 4780, Loss: 0.006103843450546265\n",
      "Epoch: 1, Batch: 4800, Loss: 0.01696878671646118\n",
      "Epoch: 1, Batch: 4820, Loss: 0.031105056405067444\n",
      "Epoch: 1, Batch: 4840, Loss: 0.2125173807144165\n",
      "Epoch: 1, Batch: 4860, Loss: 0.11993689835071564\n",
      "Epoch: 1, Batch: 4880, Loss: 0.06969639658927917\n",
      "Epoch: 1, Batch: 4900, Loss: 0.028666138648986816\n",
      "Epoch: 1, Batch: 4920, Loss: 0.09022994339466095\n",
      "Epoch: 1, Batch: 4940, Loss: 0.02249506115913391\n",
      "Epoch: 1, Batch: 4960, Loss: 0.12693993747234344\n",
      "Epoch: 1, Batch: 4980, Loss: 0.041081756353378296\n",
      "Epoch: 1, Batch: 5000, Loss: 0.2084551453590393\n",
      "Epoch: 1, Batch: 5020, Loss: 0.12705715000629425\n",
      "Epoch: 1, Batch: 5040, Loss: 0.16411730647087097\n",
      "Epoch: 1, Batch: 5060, Loss: 0.2589848041534424\n",
      "Epoch: 1, Batch: 5080, Loss: 0.04634327441453934\n",
      "Epoch: 1, Batch: 5100, Loss: 0.03375353664159775\n",
      "Epoch: 1, Batch: 5120, Loss: 0.03948467969894409\n",
      "Epoch: 1, Batch: 5140, Loss: 0.013867303729057312\n",
      "Epoch: 1, Batch: 5160, Loss: 0.07840621471405029\n",
      "Epoch: 1, Batch: 5180, Loss: 0.009583130478858948\n",
      "Epoch: 1, Batch: 5200, Loss: 0.07923278212547302\n",
      "Epoch: 1, Batch: 5220, Loss: 0.01933668553829193\n",
      "Epoch: 1, Batch: 5240, Loss: 0.035077884793281555\n",
      "Epoch: 1, Batch: 5260, Loss: 0.04565681517124176\n",
      "Epoch: 1, Batch: 5280, Loss: 0.00785607099533081\n",
      "Epoch: 1, Batch: 5300, Loss: 0.2532481849193573\n",
      "Epoch: 1, Batch: 5320, Loss: 0.162178635597229\n",
      "Epoch: 1, Batch: 5340, Loss: 0.04597657918930054\n",
      "Epoch: 1, Batch: 5360, Loss: 0.2325417846441269\n",
      "Epoch: 1, Batch: 5380, Loss: 0.008934684097766876\n",
      "Epoch: 1, Batch: 5400, Loss: 0.016578003764152527\n",
      "Epoch: 1, Batch: 5420, Loss: 0.02098224312067032\n",
      "Epoch: 1, Batch: 5440, Loss: 0.004354134202003479\n",
      "Epoch: 1, Batch: 5460, Loss: 0.07559220492839813\n",
      "Epoch: 1, Batch: 5480, Loss: 0.07268474251031876\n",
      "Epoch: 1, Batch: 5500, Loss: 0.0655050128698349\n",
      "Epoch: 1, Batch: 5520, Loss: 0.08563793450593948\n",
      "Epoch: 1, Batch: 5540, Loss: 0.0534452423453331\n",
      "Epoch: 1, Batch: 5560, Loss: 0.2507673501968384\n",
      "Epoch: 1, Batch: 5580, Loss: 0.19597244262695312\n",
      "Epoch: 1, Batch: 5600, Loss: 0.12446925789117813\n",
      "Epoch: 1, Batch: 5620, Loss: 0.04810872673988342\n",
      "Epoch: 1, Batch: 5640, Loss: 0.005824171006679535\n",
      "Epoch: 1, Batch: 5660, Loss: 0.013540402054786682\n",
      "Epoch: 1, Batch: 5680, Loss: 0.11780625581741333\n",
      "Epoch: 1, Batch: 5700, Loss: 0.050505295395851135\n",
      "Epoch: 1, Batch: 5720, Loss: 0.026369445025920868\n",
      "Epoch: 1, Batch: 5740, Loss: 0.09549739956855774\n",
      "Epoch: 1, Batch: 5760, Loss: 0.05679436773061752\n",
      "Epoch: 1, Batch: 5780, Loss: 0.046364858746528625\n",
      "Epoch: 1, Batch: 5800, Loss: 0.030080832540988922\n",
      "Epoch: 1, Batch: 5820, Loss: 0.38035261631011963\n",
      "Epoch: 1, Batch: 5840, Loss: 0.030147463083267212\n",
      "Epoch: 1, Batch: 5860, Loss: 0.022658511996269226\n",
      "Epoch: 1, Batch: 5880, Loss: 0.10184744000434875\n",
      "Epoch: 1, Batch: 5900, Loss: 0.01533333957195282\n",
      "Epoch: 1, Batch: 5920, Loss: 0.10260540246963501\n",
      "Epoch: 1, Batch: 5940, Loss: 0.018562734127044678\n",
      "Epoch: 1, Batch: 5960, Loss: 0.10672949999570847\n",
      "Epoch: 1, Batch: 5980, Loss: 0.010700486600399017\n",
      "Epoch: 1, Batch: 6000, Loss: 0.0024804621934890747\n",
      "Epoch: 1, Batch: 6020, Loss: 0.017302341759204865\n",
      "Epoch: 1, Batch: 6040, Loss: 0.14877313375473022\n",
      "Epoch: 1, Batch: 6060, Loss: 0.12001795321702957\n",
      "Epoch: 1, Batch: 6080, Loss: 0.04476211220026016\n",
      "Epoch: 1, Batch: 6100, Loss: 0.014278590679168701\n",
      "Epoch: 1, Batch: 6120, Loss: 0.08274400234222412\n",
      "Epoch: 1, Batch: 6140, Loss: 0.08692891895771027\n",
      "Epoch: 1, Batch: 6160, Loss: 0.025511503219604492\n",
      "Epoch: 1, Batch: 6180, Loss: 0.15373975038528442\n",
      "Epoch: 1, Batch: 6200, Loss: 0.025104552507400513\n",
      "Epoch: 1, Batch: 6220, Loss: 0.3185364603996277\n",
      "Epoch: 1, Batch: 6240, Loss: 0.036838918924331665\n",
      "Epoch: 1, Batch: 6260, Loss: 0.01667572557926178\n",
      "Epoch: 1, Batch: 6280, Loss: 0.5150173902511597\n",
      "Epoch: 1, Batch: 6300, Loss: 0.051742106676101685\n",
      "Epoch: 1, Batch: 6320, Loss: 0.07036682963371277\n",
      "Epoch: 1, Batch: 6340, Loss: 0.016473159193992615\n",
      "Epoch: 1, Batch: 6360, Loss: 0.05204758048057556\n",
      "Epoch: 1, Batch: 6380, Loss: 0.007917523384094238\n",
      "Epoch: 1, Batch: 6400, Loss: 0.12223589420318604\n",
      "Epoch: 1, Batch: 6420, Loss: 0.2462608963251114\n",
      "Epoch: 1, Batch: 6440, Loss: 0.0235806405544281\n",
      "Epoch: 1, Batch: 6460, Loss: 0.10794836282730103\n",
      "Epoch: 1, Batch: 6480, Loss: 0.026657581329345703\n",
      "Epoch: 1, Batch: 6500, Loss: 0.010442227125167847\n",
      "Epoch: 1, Batch: 6520, Loss: 0.066536545753479\n",
      "Epoch: 1, Batch: 6540, Loss: 0.009311497211456299\n",
      "Epoch: 1, Batch: 6560, Loss: 0.3025895059108734\n",
      "Epoch: 1, Batch: 6580, Loss: 0.03662724792957306\n",
      "Epoch: 1, Batch: 6600, Loss: 0.23786364495754242\n",
      "Epoch: 1, Batch: 6620, Loss: 0.020470254123210907\n",
      "Epoch: 1, Batch: 6640, Loss: 0.11520807445049286\n",
      "Epoch: 1, Batch: 6660, Loss: 0.18596631288528442\n",
      "Epoch: 1, Batch: 6680, Loss: 0.07136227190494537\n",
      "Epoch: 1, Batch: 6700, Loss: 0.2911609411239624\n",
      "Epoch: 1, Batch: 6720, Loss: 0.010090678930282593\n",
      "Epoch: 1, Batch: 6740, Loss: 0.03120887279510498\n",
      "Epoch: 1, Batch: 6760, Loss: 0.01133747398853302\n",
      "Epoch: 1, Batch: 6780, Loss: 0.3308602273464203\n",
      "Epoch: 1, Batch: 6800, Loss: 0.1904081106185913\n",
      "Epoch: 1, Batch: 6820, Loss: 0.05582432448863983\n",
      "Epoch: 1, Batch: 6840, Loss: 0.008440926671028137\n",
      "Epoch: 1, Batch: 6860, Loss: 0.01571793109178543\n",
      "Epoch: 1, Batch: 6880, Loss: 0.020511984825134277\n",
      "Epoch: 1, Batch: 6900, Loss: 0.056628525257110596\n",
      "Epoch: 1, Batch: 6920, Loss: 0.12819314002990723\n",
      "Epoch: 1, Batch: 6940, Loss: 0.1661761999130249\n",
      "Epoch: 1, Batch: 6960, Loss: 0.02954426407814026\n",
      "Epoch: 1, Batch: 6980, Loss: 0.017027944326400757\n",
      "Epoch: 1, Batch: 7000, Loss: 0.10877254605293274\n",
      "Epoch: 1, Batch: 7020, Loss: 0.13413089513778687\n",
      "Epoch: 1, Batch: 7040, Loss: 0.01453758031129837\n",
      "Epoch: 1, Batch: 7060, Loss: 0.10092654079198837\n",
      "Epoch: 1, Batch: 7080, Loss: 0.05066034197807312\n",
      "Epoch: 1, Batch: 7100, Loss: 0.0871296375989914\n",
      "Epoch: 1, Batch: 7120, Loss: 0.04367542266845703\n",
      "Epoch: 1, Batch: 7140, Loss: 0.04320836067199707\n",
      "Epoch: 1, Batch: 7160, Loss: 0.02442765235900879\n",
      "Epoch: 1, Batch: 7180, Loss: 0.07949486374855042\n",
      "Epoch: 1, Batch: 7200, Loss: 0.06837671995162964\n",
      "Epoch: 1, Batch: 7220, Loss: 0.04965381324291229\n",
      "Epoch: 1, Batch: 7240, Loss: 0.012161821126937866\n",
      "Epoch: 1, Batch: 7260, Loss: 0.07887153327465057\n",
      "Epoch: 1, Batch: 7280, Loss: 0.2599831223487854\n",
      "Epoch: 1, Batch: 7300, Loss: 0.007678329944610596\n",
      "Epoch: 1, Batch: 7320, Loss: 0.28820139169692993\n",
      "Epoch: 1, Batch: 7340, Loss: 0.1659153401851654\n",
      "Epoch: 1, Batch: 7360, Loss: 0.20512723922729492\n",
      "Epoch: 1, Batch: 7380, Loss: 0.13409212231636047\n",
      "Epoch: 1, Batch: 7400, Loss: 0.007963493466377258\n",
      "Epoch: 1, Batch: 7420, Loss: 0.08037403225898743\n",
      "Epoch: 1, Batch: 7440, Loss: 0.0870451033115387\n",
      "Epoch: 1, Batch: 7460, Loss: 0.15222285687923431\n",
      "Epoch: 1, Batch: 7480, Loss: 0.10456131398677826\n",
      "End of Epoch 1, Average Loss: 0.11739569888185802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./my_fine_tuned_bert\\\\tokenizer_config.json',\n",
       " './my_fine_tuned_bert\\\\special_tokens_map.json',\n",
       " './my_fine_tuned_bert\\\\vocab.txt',\n",
       " './my_fine_tuned_bert\\\\added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, data_loader, optimizer, device)\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained('./my_fine_tuned_bert')\n",
    "tokenizer.save_pretrained('./my_fine_tuned_bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('./my_fine_tuned_bert')\n",
    "model = BertForSequenceClassification.from_pretrained('./my_fine_tuned_bert', num_labels=2)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preprocessed_data_test(tokenized_inputs, file_path='preprocessed_data.pt'):\n",
    "    torch.save({\n",
    "        'input_ids': tokenized_inputs['input_ids'],\n",
    "        'attention_mask': tokenized_inputs['attention_mask']\n",
    "    }, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "test_data = load_data('data\\SemEval2024-Task8-test\\subtaskA_monolingual.jsonl')\n",
    "tokenized_test = preprocess_data(test_data, tokenizer)\n",
    "#labels = torch.tensor(test_data['label'].tolist()).unsqueeze(1)\n",
    "\n",
    "# Save the preprocessed data\n",
    "save_preprocessed_data_test(tokenized_test, 'preprocessed_test_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDataset and DataLoader\n",
    "preprocessed_data = load_preprocessed_data('preprocessed_test_data.pt')\n",
    "test_dataset = TensorDataset(preprocessed_data['input_ids'], preprocessed_data['attention_mask'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "def generate_predictions(model, dataloader):\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {'input_ids': batch[0].to(device),\n",
    "                      'attention_mask': batch[1].to(device)}\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    return predictions\n",
    "\n",
    "predictions = generate_predictions(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to JSONL for scoring\n",
    "def save_predictions(predictions, data, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for idx, pred in enumerate(predictions):\n",
    "            result = {\n",
    "                \"id\": str(data.iloc[idx]['id']),  # Convert ID to string if needed\n",
    "                \"label\": int(pred)  # Ensure label is a Python int\n",
    "            }\n",
    "            f.write(json.dumps(result) + \"\\n\")\n",
    "\n",
    "save_predictions(predictions, test_data, 'BERT_SubtaskA_mono_results.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7068\n",
      "Precision: 0.882396449704142\n",
      "Recall: 0.4772\n",
      "F1 Score: 0.6194184839044652\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Print evaluation metrics (optional, for immediate insight) on the dev dataset\n",
    "def print_evaluation_metrics(predictions, true_labels):\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "\n",
    "print_evaluation_metrics(predictions, test_data['label'].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
