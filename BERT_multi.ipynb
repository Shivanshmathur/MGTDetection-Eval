{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to load the dataset from a JSONL file\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load tokenizer and model with correct number of labels for Subtask B\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "num_labels = 6  # Update this based on the actual number of classes in Subtask B\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and tokenize the data\n",
    "def preprocess_data(data, tokenizer, max_length=512):\n",
    "    tokenized = tokenizer(data['text'].tolist(), padding='max_length', max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare Subtask B training data\n",
    "train_data = load_data('data\\SubtaskB\\subtaskB_train.jsonl')  # Update with correct path\n",
    "tokenized_inputs = preprocess_data(train_data, tokenizer)\n",
    "labels = torch.tensor(train_data['label'].tolist())\n",
    "\n",
    "# Modify DataLoader setup to handle the correct data\n",
    "dataset = TensorDataset(tokenized_inputs['input_ids'], tokenized_inputs['attention_mask'], labels)\n",
    "data_loader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preprocessed_data(tokenized_inputs, labels, file_path='preprocessed_data_train_subtaskB.pt'):\n",
    "    torch.save({\n",
    "        'input_ids': tokenized_inputs['input_ids'],\n",
    "        'attention_mask': tokenized_inputs['attention_mask'],\n",
    "        'labels': labels\n",
    "    }, file_path)\n",
    "\n",
    "# Save the preprocessed data\n",
    "save_preprocessed_data(tokenized_inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shivansh Mathur\\.conda\\envs\\dlproj2\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define optimizer and scaler for training\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, data_loader, optimizer, device, epochs=1):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 20 == 0:  # Logging for visibility\n",
    "                print(f'Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {loss.item()}')\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f'End of Epoch {epoch+1}, Average Loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 0, Loss: 1.80242919921875\n",
      "Epoch: 1, Batch: 20, Loss: 1.7625732421875\n",
      "Epoch: 1, Batch: 40, Loss: 1.2254180908203125\n",
      "Epoch: 1, Batch: 60, Loss: 1.4940338134765625\n",
      "Epoch: 1, Batch: 80, Loss: 1.1616249084472656\n",
      "Epoch: 1, Batch: 100, Loss: 1.0655441284179688\n",
      "Epoch: 1, Batch: 120, Loss: 0.9373207092285156\n",
      "Epoch: 1, Batch: 140, Loss: 0.9362010955810547\n",
      "Epoch: 1, Batch: 160, Loss: 0.8044834136962891\n",
      "Epoch: 1, Batch: 180, Loss: 1.2731742858886719\n",
      "Epoch: 1, Batch: 200, Loss: 0.5685396194458008\n",
      "Epoch: 1, Batch: 220, Loss: 0.7308998107910156\n",
      "Epoch: 1, Batch: 240, Loss: 0.8597469329833984\n",
      "Epoch: 1, Batch: 260, Loss: 0.9117484092712402\n",
      "Epoch: 1, Batch: 280, Loss: 0.5755653381347656\n",
      "Epoch: 1, Batch: 300, Loss: 0.5395126342773438\n",
      "Epoch: 1, Batch: 320, Loss: 0.9269037246704102\n",
      "Epoch: 1, Batch: 340, Loss: 0.8286418914794922\n",
      "Epoch: 1, Batch: 360, Loss: 0.5047273635864258\n",
      "Epoch: 1, Batch: 380, Loss: 0.27229976654052734\n",
      "Epoch: 1, Batch: 400, Loss: 0.7296180725097656\n",
      "Epoch: 1, Batch: 420, Loss: 0.47525978088378906\n",
      "Epoch: 1, Batch: 440, Loss: 0.29134464263916016\n",
      "Epoch: 1, Batch: 460, Loss: 0.27541017532348633\n",
      "Epoch: 1, Batch: 480, Loss: 0.29624366760253906\n",
      "Epoch: 1, Batch: 500, Loss: 0.7207393646240234\n",
      "Epoch: 1, Batch: 520, Loss: 0.24050498008728027\n",
      "Epoch: 1, Batch: 540, Loss: 1.0500335693359375\n",
      "Epoch: 1, Batch: 560, Loss: 0.5078868865966797\n",
      "Epoch: 1, Batch: 580, Loss: 0.9338550567626953\n",
      "Epoch: 1, Batch: 600, Loss: 0.6575314998626709\n",
      "Epoch: 1, Batch: 620, Loss: 0.3846890926361084\n",
      "Epoch: 1, Batch: 640, Loss: 0.28139543533325195\n",
      "Epoch: 1, Batch: 660, Loss: 0.28678202629089355\n",
      "Epoch: 1, Batch: 680, Loss: 0.20176172256469727\n",
      "Epoch: 1, Batch: 700, Loss: 0.6747586727142334\n",
      "Epoch: 1, Batch: 720, Loss: 0.36248111724853516\n",
      "Epoch: 1, Batch: 740, Loss: 0.44838881492614746\n",
      "Epoch: 1, Batch: 760, Loss: 0.16031265258789062\n",
      "Epoch: 1, Batch: 780, Loss: 0.3847006559371948\n",
      "Epoch: 1, Batch: 800, Loss: 0.6935200691223145\n",
      "Epoch: 1, Batch: 820, Loss: 0.13465476036071777\n",
      "Epoch: 1, Batch: 840, Loss: 0.2851351499557495\n",
      "Epoch: 1, Batch: 860, Loss: 0.20641839504241943\n",
      "Epoch: 1, Batch: 880, Loss: 0.14380335807800293\n",
      "Epoch: 1, Batch: 900, Loss: 0.5333242416381836\n",
      "Epoch: 1, Batch: 920, Loss: 0.051017165184020996\n",
      "Epoch: 1, Batch: 940, Loss: 0.16075348854064941\n",
      "Epoch: 1, Batch: 960, Loss: 0.4496561288833618\n",
      "Epoch: 1, Batch: 980, Loss: 0.24116241931915283\n",
      "Epoch: 1, Batch: 1000, Loss: 0.11584937572479248\n",
      "Epoch: 1, Batch: 1020, Loss: 0.21678543090820312\n",
      "Epoch: 1, Batch: 1040, Loss: 0.32123279571533203\n",
      "Epoch: 1, Batch: 1060, Loss: 0.2704617977142334\n",
      "Epoch: 1, Batch: 1080, Loss: 0.533871054649353\n",
      "Epoch: 1, Batch: 1100, Loss: 0.765265941619873\n",
      "Epoch: 1, Batch: 1120, Loss: 0.33147621154785156\n",
      "Epoch: 1, Batch: 1140, Loss: 0.2413640022277832\n",
      "Epoch: 1, Batch: 1160, Loss: 0.25646841526031494\n",
      "Epoch: 1, Batch: 1180, Loss: 0.7464132308959961\n",
      "Epoch: 1, Batch: 1200, Loss: 0.37536031007766724\n",
      "Epoch: 1, Batch: 1220, Loss: 0.4889497756958008\n",
      "Epoch: 1, Batch: 1240, Loss: 0.23222637176513672\n",
      "Epoch: 1, Batch: 1260, Loss: 0.29062867164611816\n",
      "Epoch: 1, Batch: 1280, Loss: 0.14799284934997559\n",
      "Epoch: 1, Batch: 1300, Loss: 0.08736252784729004\n",
      "Epoch: 1, Batch: 1320, Loss: 0.3366633653640747\n",
      "Epoch: 1, Batch: 1340, Loss: 0.24070674180984497\n",
      "Epoch: 1, Batch: 1360, Loss: 0.07407748699188232\n",
      "Epoch: 1, Batch: 1380, Loss: 0.9954383969306946\n",
      "Epoch: 1, Batch: 1400, Loss: 0.6315664649009705\n",
      "Epoch: 1, Batch: 1420, Loss: 0.12594902515411377\n",
      "Epoch: 1, Batch: 1440, Loss: 0.14511680603027344\n",
      "Epoch: 1, Batch: 1460, Loss: 0.20930814743041992\n",
      "Epoch: 1, Batch: 1480, Loss: 0.4490562081336975\n",
      "Epoch: 1, Batch: 1500, Loss: 0.2791883945465088\n",
      "Epoch: 1, Batch: 1520, Loss: 0.4258362650871277\n",
      "Epoch: 1, Batch: 1540, Loss: 0.39881473779678345\n",
      "Epoch: 1, Batch: 1560, Loss: 0.3315606117248535\n",
      "Epoch: 1, Batch: 1580, Loss: 0.2994716167449951\n",
      "Epoch: 1, Batch: 1600, Loss: 0.7002561092376709\n",
      "Epoch: 1, Batch: 1620, Loss: 0.6236236691474915\n",
      "Epoch: 1, Batch: 1640, Loss: 0.29019641876220703\n",
      "Epoch: 1, Batch: 1660, Loss: 0.18031835556030273\n",
      "Epoch: 1, Batch: 1680, Loss: 0.15893930196762085\n",
      "Epoch: 1, Batch: 1700, Loss: 0.4533812999725342\n",
      "Epoch: 1, Batch: 1720, Loss: 0.1545654833316803\n",
      "Epoch: 1, Batch: 1740, Loss: 0.642417848110199\n",
      "Epoch: 1, Batch: 1760, Loss: 0.14961814880371094\n",
      "Epoch: 1, Batch: 1780, Loss: 0.30204468965530396\n",
      "Epoch: 1, Batch: 1800, Loss: 0.6097228527069092\n",
      "Epoch: 1, Batch: 1820, Loss: 0.24765372276306152\n",
      "Epoch: 1, Batch: 1840, Loss: 0.627739667892456\n",
      "Epoch: 1, Batch: 1860, Loss: 0.09072619676589966\n",
      "Epoch: 1, Batch: 1880, Loss: 0.7989904880523682\n",
      "Epoch: 1, Batch: 1900, Loss: 0.33138710260391235\n",
      "Epoch: 1, Batch: 1920, Loss: 0.22526907920837402\n",
      "Epoch: 1, Batch: 1940, Loss: 0.3821859359741211\n",
      "Epoch: 1, Batch: 1960, Loss: 0.623423159122467\n",
      "Epoch: 1, Batch: 1980, Loss: 0.13646548986434937\n",
      "Epoch: 1, Batch: 2000, Loss: 0.18325984477996826\n",
      "Epoch: 1, Batch: 2020, Loss: 0.1128852367401123\n",
      "Epoch: 1, Batch: 2040, Loss: 0.13995474576950073\n",
      "Epoch: 1, Batch: 2060, Loss: 0.24795177578926086\n",
      "Epoch: 1, Batch: 2080, Loss: 0.23727047443389893\n",
      "Epoch: 1, Batch: 2100, Loss: 0.4703463613986969\n",
      "Epoch: 1, Batch: 2120, Loss: 0.29553210735321045\n",
      "Epoch: 1, Batch: 2140, Loss: 0.1317906379699707\n",
      "Epoch: 1, Batch: 2160, Loss: 0.35773617029190063\n",
      "Epoch: 1, Batch: 2180, Loss: 0.38074958324432373\n",
      "Epoch: 1, Batch: 2200, Loss: 0.4936981797218323\n",
      "Epoch: 1, Batch: 2220, Loss: 0.29780101776123047\n",
      "Epoch: 1, Batch: 2240, Loss: 0.2260965257883072\n",
      "Epoch: 1, Batch: 2260, Loss: 0.30063530802726746\n",
      "Epoch: 1, Batch: 2280, Loss: 0.20768868923187256\n",
      "Epoch: 1, Batch: 2300, Loss: 0.34621739387512207\n",
      "Epoch: 1, Batch: 2320, Loss: 0.13861697912216187\n",
      "Epoch: 1, Batch: 2340, Loss: 0.10519775748252869\n",
      "Epoch: 1, Batch: 2360, Loss: 0.08935639262199402\n",
      "Epoch: 1, Batch: 2380, Loss: 0.29322025179862976\n",
      "Epoch: 1, Batch: 2400, Loss: 0.30588120222091675\n",
      "Epoch: 1, Batch: 2420, Loss: 0.1821466088294983\n",
      "Epoch: 1, Batch: 2440, Loss: 0.23856371641159058\n",
      "Epoch: 1, Batch: 2460, Loss: 0.32887160778045654\n",
      "Epoch: 1, Batch: 2480, Loss: 0.1587345004081726\n",
      "Epoch: 1, Batch: 2500, Loss: 0.30518192052841187\n",
      "Epoch: 1, Batch: 2520, Loss: 0.0957251787185669\n",
      "Epoch: 1, Batch: 2540, Loss: 0.12242627143859863\n",
      "Epoch: 1, Batch: 2560, Loss: 0.4824850559234619\n",
      "Epoch: 1, Batch: 2580, Loss: 0.3631138503551483\n",
      "Epoch: 1, Batch: 2600, Loss: 0.5752468705177307\n",
      "Epoch: 1, Batch: 2620, Loss: 0.11492085456848145\n",
      "Epoch: 1, Batch: 2640, Loss: 0.24977952241897583\n",
      "Epoch: 1, Batch: 2660, Loss: 0.3226827085018158\n",
      "Epoch: 1, Batch: 2680, Loss: 0.17693427205085754\n",
      "Epoch: 1, Batch: 2700, Loss: 0.3537867069244385\n",
      "Epoch: 1, Batch: 2720, Loss: 0.10772198438644409\n",
      "Epoch: 1, Batch: 2740, Loss: 0.0842319130897522\n",
      "Epoch: 1, Batch: 2760, Loss: 0.19292980432510376\n",
      "Epoch: 1, Batch: 2780, Loss: 0.4907122254371643\n",
      "Epoch: 1, Batch: 2800, Loss: 0.4355839192867279\n",
      "Epoch: 1, Batch: 2820, Loss: 0.8219255805015564\n",
      "Epoch: 1, Batch: 2840, Loss: 0.17967072129249573\n",
      "Epoch: 1, Batch: 2860, Loss: 0.41002094745635986\n",
      "Epoch: 1, Batch: 2880, Loss: 0.39902085065841675\n",
      "Epoch: 1, Batch: 2900, Loss: 0.10855260491371155\n",
      "Epoch: 1, Batch: 2920, Loss: 0.047280848026275635\n",
      "Epoch: 1, Batch: 2940, Loss: 0.29168054461479187\n",
      "Epoch: 1, Batch: 2960, Loss: 0.32533013820648193\n",
      "Epoch: 1, Batch: 2980, Loss: 0.26959511637687683\n",
      "Epoch: 1, Batch: 3000, Loss: 0.025342702865600586\n",
      "Epoch: 1, Batch: 3020, Loss: 0.04412934184074402\n",
      "Epoch: 1, Batch: 3040, Loss: 0.28660792112350464\n",
      "Epoch: 1, Batch: 3060, Loss: 0.22686393558979034\n",
      "Epoch: 1, Batch: 3080, Loss: 0.10126450657844543\n",
      "Epoch: 1, Batch: 3100, Loss: 0.73694908618927\n",
      "Epoch: 1, Batch: 3120, Loss: 0.6846207976341248\n",
      "Epoch: 1, Batch: 3140, Loss: 0.21352680027484894\n",
      "Epoch: 1, Batch: 3160, Loss: 0.10019463300704956\n",
      "Epoch: 1, Batch: 3180, Loss: 0.23015841841697693\n",
      "Epoch: 1, Batch: 3200, Loss: 0.06564465165138245\n",
      "Epoch: 1, Batch: 3220, Loss: 0.1607462763786316\n",
      "Epoch: 1, Batch: 3240, Loss: 0.11099618673324585\n",
      "Epoch: 1, Batch: 3260, Loss: 0.34396424889564514\n",
      "Epoch: 1, Batch: 3280, Loss: 0.336204469203949\n",
      "Epoch: 1, Batch: 3300, Loss: 0.326346218585968\n",
      "Epoch: 1, Batch: 3320, Loss: 0.3355746269226074\n",
      "Epoch: 1, Batch: 3340, Loss: 0.34107470512390137\n",
      "Epoch: 1, Batch: 3360, Loss: 0.09289729595184326\n",
      "Epoch: 1, Batch: 3380, Loss: 0.29950135946273804\n",
      "Epoch: 1, Batch: 3400, Loss: 0.4238952696323395\n",
      "Epoch: 1, Batch: 3420, Loss: 0.44688546657562256\n",
      "Epoch: 1, Batch: 3440, Loss: 0.6413019895553589\n",
      "Epoch: 1, Batch: 3460, Loss: 0.29519540071487427\n",
      "Epoch: 1, Batch: 3480, Loss: 0.3725811839103699\n",
      "Epoch: 1, Batch: 3500, Loss: 0.32440251111984253\n",
      "Epoch: 1, Batch: 3520, Loss: 0.4037935137748718\n",
      "Epoch: 1, Batch: 3540, Loss: 0.28175652027130127\n",
      "Epoch: 1, Batch: 3560, Loss: 0.24586814641952515\n",
      "Epoch: 1, Batch: 3580, Loss: 0.3012657165527344\n",
      "Epoch: 1, Batch: 3600, Loss: 0.060337722301483154\n",
      "Epoch: 1, Batch: 3620, Loss: 0.3033132553100586\n",
      "Epoch: 1, Batch: 3640, Loss: 0.4447493851184845\n",
      "Epoch: 1, Batch: 3660, Loss: 0.9043847322463989\n",
      "Epoch: 1, Batch: 3680, Loss: 0.2844407558441162\n",
      "Epoch: 1, Batch: 3700, Loss: 0.33907464146614075\n",
      "Epoch: 1, Batch: 3720, Loss: 0.37441039085388184\n",
      "Epoch: 1, Batch: 3740, Loss: 0.358992338180542\n",
      "Epoch: 1, Batch: 3760, Loss: 0.5763388872146606\n",
      "Epoch: 1, Batch: 3780, Loss: 0.3958735466003418\n",
      "Epoch: 1, Batch: 3800, Loss: 0.855562686920166\n",
      "Epoch: 1, Batch: 3820, Loss: 0.27837443351745605\n",
      "Epoch: 1, Batch: 3840, Loss: 0.08292141556739807\n",
      "Epoch: 1, Batch: 3860, Loss: 0.4894687533378601\n",
      "Epoch: 1, Batch: 3880, Loss: 0.0932970643043518\n",
      "Epoch: 1, Batch: 3900, Loss: 0.13312602043151855\n",
      "Epoch: 1, Batch: 3920, Loss: 0.39810678362846375\n",
      "Epoch: 1, Batch: 3940, Loss: 0.43092799186706543\n",
      "Epoch: 1, Batch: 3960, Loss: 0.17445777356624603\n",
      "Epoch: 1, Batch: 3980, Loss: 0.6296395659446716\n",
      "Epoch: 1, Batch: 4000, Loss: 0.19276462495326996\n",
      "Epoch: 1, Batch: 4020, Loss: 0.25781625509262085\n",
      "Epoch: 1, Batch: 4040, Loss: 0.15143273770809174\n",
      "Epoch: 1, Batch: 4060, Loss: 0.048892661929130554\n",
      "Epoch: 1, Batch: 4080, Loss: 0.2182445526123047\n",
      "Epoch: 1, Batch: 4100, Loss: 0.20495478808879852\n",
      "Epoch: 1, Batch: 4120, Loss: 0.20631620287895203\n",
      "Epoch: 1, Batch: 4140, Loss: 0.02712687849998474\n",
      "Epoch: 1, Batch: 4160, Loss: 0.09963177144527435\n",
      "Epoch: 1, Batch: 4180, Loss: 0.18173746764659882\n",
      "Epoch: 1, Batch: 4200, Loss: 0.12479238212108612\n",
      "Epoch: 1, Batch: 4220, Loss: 0.08700376749038696\n",
      "Epoch: 1, Batch: 4240, Loss: 0.08443114906549454\n",
      "Epoch: 1, Batch: 4260, Loss: 0.11012022197246552\n",
      "Epoch: 1, Batch: 4280, Loss: 0.1590626835823059\n",
      "Epoch: 1, Batch: 4300, Loss: 0.3185679316520691\n",
      "Epoch: 1, Batch: 4320, Loss: 0.06701791286468506\n",
      "Epoch: 1, Batch: 4340, Loss: 0.16679207980632782\n",
      "Epoch: 1, Batch: 4360, Loss: 0.15551266074180603\n",
      "Epoch: 1, Batch: 4380, Loss: 0.3948512077331543\n",
      "Epoch: 1, Batch: 4400, Loss: 0.33459192514419556\n",
      "Epoch: 1, Batch: 4420, Loss: 0.5579418540000916\n",
      "End of Epoch 1, Average Loss: 0.3789007670791434\n"
     ]
    }
   ],
   "source": [
    "train_model(model, data_loader, optimizer, device)\n",
    "\n",
    "# Saving the trained model\n",
    "model.save_pretrained('./my_fine_tuned_bert_multiclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my_fine_tuned_bert_multiclass\\\\tokenizer_config.json',\n",
       " './my_fine_tuned_bert_multiclass\\\\special_tokens_map.json',\n",
       " './my_fine_tuned_bert_multiclass\\\\vocab.txt',\n",
       " './my_fine_tuned_bert_multiclass\\\\added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('./my_fine_tuned_bert_multiclass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model for testing\n",
    "tokenizer = BertTokenizer.from_pretrained('./my_fine_tuned_bert_multiclass')\n",
    "model = BertForSequenceClassification.from_pretrained('./my_fine_tuned_bert_multiclass', num_labels=6)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load and prepare test data\n",
    "test_data = load_data('data\\SubtaskB\\subtaskB_dev.jsonl')  # Update with correct path\n",
    "tokenized_test = preprocess_data(test_data, tokenizer)\n",
    "test_dataset = TensorDataset(tokenized_test['input_ids'], tokenized_test['attention_mask'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate predictions\n",
    "def generate_predictions(model, dataloader):\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {'input_ids': batch[0].to(device),\n",
    "                      'attention_mask': batch[1].to(device)}\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    return predictions\n",
    "\n",
    "predictions = generate_predictions(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6213333333333333\n",
      "Precision: 0.6440920563111239\n",
      "Recall: 0.6213333333333333\n",
      "F1 Score: 0.6129514471012584\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have the true labels for evaluation\n",
    "test_labels = torch.tensor(test_data['label'].tolist())  # This assumes labels are present in your test data JSONL\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "# Print evaluation metrics\n",
    "def print_evaluation_metrics(predictions, true_labels):\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "\n",
    "print_evaluation_metrics(predictions, test_labels.numpy())\n",
    "\n",
    "# Optional: Save predictions to a JSONL file for further analysis or submission\n",
    "def save_predictions(predictions, data, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for idx, pred in enumerate(predictions):\n",
    "            result = {\n",
    "                \"id\": str(data.iloc[idx]['id']),  # Convert ID to string if needed\n",
    "                \"label\": int(pred)  # Ensure label is a Python int\n",
    "            }\n",
    "            f.write(json.dumps(result) + \"\\n\")\n",
    "\n",
    "save_predictions(predictions, test_data, 'BERT_SubtaskB_results.jsonl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
